{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMHCqCGlOAji0DpxQZrh4eO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":750},"id":"WkkW64ObjpQQ","executionInfo":{"status":"ok","timestamp":1751039040508,"user_tz":-60,"elapsed":3987,"user":{"displayName":"Emmanuel Aghedo","userId":"13474803354197682539"}},"outputId":"780ce52c-8863-44fa-8761-b6f01074c4d0"},"outputs":[{"output_type":"stream","name":"stderr","text":["If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n","Some weights of BertLMHeadModel were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Device set to use cuda:0\n","/tmp/ipython-input-3-930022607.py:25: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n","  chatbot = gr.Chatbot()\n"]},{"output_type":"stream","name":"stdout","text":["It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://ce2895a69a2d0b733a.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://ce2895a69a2d0b733a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":3}],"source":["import gradio as gr\n","from transformers import pipeline\n","\n","# Load your model\n","pipe = pipeline(\"text-generation\", model=\"sentence-transformers/all-MiniLM-L6-v2\", device=0)\n","#pipe = pipeline(\"text-generation\", model=\"tiiuae/falcon-7b-instruct\", device=0)\n","# Simple context retriever (replace with your actual semantic retriever)\n","def retrieve_context(query):\n","    # Load markdown content or mock for demo\n","    markdown_folder = \"/content/drive/MyDrive/Colab Notebooks/UWE Data.md\"\n","    return \"This is a placeholder context from markdown files.\"\n","\n","chat_history = []\n","\n","def chat_interface(query):\n","    context = retrieve_context(query)\n","    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n","    output = pipe(prompt, max_new_tokens=256, do_sample=False)[0][\"generated_text\"]\n","    answer = output.split(\"Answer:\")[-1].strip()\n","    chat_history.append((query, answer))\n","    return chat_history\n","\n","with gr.Blocks() as demo:\n","    gr.Markdown(\"## ðŸ¤– UWE RAG Chatbot\")\n","    chatbot = gr.Chatbot()\n","    msg = gr.Textbox(label=\"Ask a question\")\n","    clear = gr.Button(\"Clear Chat\")\n","\n","    def respond(message):\n","        chat = chat_interface(message)\n","        return \"\", chat\n","\n","    msg.submit(respond, [msg], [msg, chatbot])\n","    clear.click(lambda: [], None, chatbot)\n","\n","demo.launch()\n"]},{"cell_type":"code","source":[],"metadata":{"id":"puGB15Iejxsh"},"execution_count":null,"outputs":[]}]}